"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"InspectAR: An Augmented Reality Inspection Framework for Industry","R. Perla; G. Gupta; R. Hebbalaguppe; E. Hassan","TCS Research, Delhi, India; TCS Research, Delhi, India; TCS Research, Delhi, India; TCS Research, Delhi, India","2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)","2 Feb 2017","2016","","","355","356","With the advancement in camera technologies and data streaming protocols, AR based applications are proving to be an important aid for inspection, training and supervision tasks in various operations including automotive industry, education etc. We demonstrate an AR based re-configurable inspection framework that can be utilized in cross-domain applications such as maintenance and repair assistance in industrial inspection and automotive/avionics domain inspection, amongst others. A deep learning component detects parts viewed in inspector's Field-of-View (FoV) accurately and the corresponding inspection check-list can be prioritized based on detection results. The back-end of the framework is easily configurable for different applications where instructions can be directly imported and visually integrated with inspection type. Accurate recording of status of inspection is provided through evidence capturing of images, notes and videos. Our current framework supports all the Android based devices and will be demonstrated on Google Glass, Google Cardboard with smartphone, and Tablet with the help of 3D printer inspection use-case.","","978-1-5090-3740-7","10.1109/ISMAR-Adjunct.2016.0119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836540","H.5.1 [Information Interfaces and Presentation]: Artificial;Augmented;and Virtual Realities—; [Human-centered computing]: Ubiquitous and mobile computing—Ambient Intelligence I.4.8 [Computing Methodologies]: Image Processing and Computer Vi","Inspection;Google;Servers;Glass;Object detection;Augmented reality;Printers","Android (operating system);augmented reality;automatic optical inspection;image capture;learning (artificial intelligence);printers;production engineering computing;smart phones;three-dimensional printing","augmented reality inspection;InspectAR;camera technologies;data streaming protocols;AR based applications;AR based reconfigurable inspection;cross-domain applications;deep learning component;inspector field-of-view;inspection check-list;part detection;inspection type;evidence image capturing;Android based devices;Google Glass;Google Cardboard;smartphone;Tablet;3D printer inspection use-case","","12","1","3","IEEE","2 Feb 2017","","","IEEE","IEEE Conferences"
"Research on Augmented Reality Technology in the Training of Pre-flight Safety Inspect Process","W. Ye; N. He; J. Wang","R&D Center, The Second Research Institute of CAAC, Chengdu, China; R&D Center, The Second Research Institute of CAAC, Chengdu, China; R&D Center, The Second Research Institute of CAAC, Chengdu, China","2022 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)","23 May 2022","2022","","","68","71","Traditional pre-flight safety training mainly relies on paper manuals or manual teaching, which is inefficient and of low quality, cannot meet the current trend of urgent introducing aircraft inspection personnel. In the inspect process, there is no real-time information support and guidance, and the difficult problems on the site cannot be solved in time, resulting in low inspect efficiency, but also easy to induce technical errors caused by human factors, such as the inspect personnel carelessly missed a certain step during the inspection process, that will result in many adverse consequences. Based on the current situation of civil aviation inspection industry, the pre-flight inspect process work is complexity, training technicians to efficiently perform new skills is challenging. fortunately, training of this type can be supported by Augmented Reality, a powerful industrial training technology that directly links instructions on how to perform the service tasks to the aircraft components that require processing. Meanwhile, because of the increasing complexity of inspection tasks, it is not sufficient to train the technicians in task execution. It is not sufficient to train the technicians in task execution. an effective solution with the help of visualization technology is proposed. It is mainly to digitize paper manual materials into text information, pictures, videos, 3D models and other multimedia means, combined with human computer interaction development, efficient inspection training services and real-time reliable inspection guidance services. Based on Unity 3D engine and with the help of Microsoft Hololens2, the augmented reality aircraft per-flight inspection training system pursues high-precision process guidance.","","978-1-6654-0902-5","10.1109/IPEC54454.2022.9777423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777423","Augmented Reality;Training;Inspection;Aviation;Graphical user interface","Training;Three-dimensional displays;Manuals;Inspection;Real-time systems;Safety;Personnel","aircraft;aircraft maintenance;augmented reality;computer based training;human factors;inspection","human factors;aircraft inspection personnel;pre-flight safety training;powerful industrial training technology;technician training;pre-flight inspect process;civil aviation inspection industry;inspect efficiency;real-time information support;manual teaching;paper manuals;high-precision process guidance;augmented reality aircraft per-flight inspection training system;real-time reliable inspection guidance services;task execution;inspection tasks","","1","","19","IEEE","23 May 2022","","","IEEE","IEEE Conferences"
"Application of Augmented Reality for Aviation Equipment Inspection and Maintenance Training","C. -C. Peng; A. -C. Chang; Y. -L. Chu","Department of Aeronautics and Astronautics, National Cheng Kung University, Tainan, Taiwan; Department of Aeronautics and Astronautics, National Cheng Kung University, Tainan, Taiwan; Department of Aeronautics and Astronautics, National Cheng Kung University, Tainan, Taiwan","2022 8th International Conference on Applied System Innovation (ICASI)","19 May 2022","2022","","","58","63","In the procedure of aviation equipment maintenance, a healthy status and prior knowledge are essential to every person. Making the newcomers get familiar with their tasks rapidly and reducing human error are important issues. Augmented reality (AR) integrated with the popular artificial intelligence (AI) technology can provide smart system inspections and train maintenance professionals on how to perform important maintenance procedures effectively and accurately. Utilization of the AR not only replaces classic manual training but also provides high mobility pre-training opportunities. In other words, AR is capable of taking equipment maintenance to the next level. In this paper, an application for aviation equipment inspection is developed. By using AR and AI, traditional standard operation procedures (SOP) can be visualized and standardized. Therefore, people who are not familiar with the maintenance procedure can still finish the standard inspections. The developed software gives a great contribution to human resources for maintenance training. Demonstrations including turbofan and landing gear are provided by using HoloLens 2 to illustrate the application novelty. Finally, pros and cons regarding maintenance using AR are also discussed.","2768-4156","978-1-6654-9650-6","10.1109/ICASI55125.2022.9774490","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774490","Augmented reality;maintenance;HoloLens 2","Training;Costs;Gears;Maintenance engineering;Inspection;Software;Artificial intelligence","aerospace computing;artificial intelligence;augmented reality;inspection;maintenance engineering;mechanical engineering computing","maintenance training;application novelty;augmented reality;aviation equipment inspection;aviation equipment maintenance;healthy status;human error;artificial intelligence technology;AI;smart system inspections;train maintenance professionals;high mobility pre-training opportunities;maintenance procedure;standard inspections","","","","14","IEEE","19 May 2022","","","IEEE","IEEE Conferences"
"Application of Augmented Reality for Equipment Maintenance and Employee Training in Manufacturing Plant","H. -H. Hsu; C. -Y. Chuang","Department of Multimedia and Game Development, Chia Nan University of Pharmacy & Science, Tainan, Taiwan; HAMASTAR Technology Co., Ltd., Kaohsiung, Taiwan","2022 IEEE 4th Eurasia Conference on Biomedical Engineering, Healthcare and Sustainability (ECBIOS)","17 Nov 2022","2022","","","136","139","The industrial design development of AR devices provides a convenient way to use AR technology for production, training, and inspection. In this paper, the application of augmented reality technology is explored for industrial purposes through the professional engines, Unity and Vuforia SDK, for Android app development. A manufacturing plant is used as an example, and preventive equipment maintenance is achieved by an instruction presentation on a display. By using AR technology, new employees with average ability and little experience can accurately perform various on-site maintenance operations, which helps learn technical experience.","","978-1-7281-9579-7","10.1109/ECBIOS54627.2022.9945026","Ministry of Economic Affairs; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945026","Augmented Reality;AR;Training;Maintenance","Training;Printing;Maintenance engineering;Production facilities;Mobile handsets;Manufacturing;Resource management","Android (operating system);augmented reality;computer based training;continuing professional development;industrial plants;industrial training;inspection;personnel;preventive maintenance;production engineering computing;production equipment","Android app development;AR devices;augmented reality;employee training;industrial design development;inspection;instruction presentation;manufacturing plant;on-site maintenance operations;preventive equipment maintenance;professional engines;technical experience;Unity;Vuforia SDK","","1","","6","IEEE","17 Nov 2022","","","IEEE","IEEE Conferences"
"Leveraging the strength of mixed reality for maintenance training -- A case study in large pumped storage power station","C. Wen; H. Ji; M. Hou; X. Qiu; Y. Zhao; J. Zou; H. Shen; Z. Yang","CSG POWER GENERATION CO., LTD MAINTENANCE AND TEST BRANCH, Guangzhou, China; CSG Power Generation Co.,Ltd Qingyuan Pumped Storage Power Generation Co.,Ltd, Guangzhou, China; CSG Power Generation Co.,Ltd Qingyuan Pumped Storage Power Generation Co.,Ltd, Guangzhou, China; CSG POWER GENERATION CO., LTD MAINTENANCE AND TEST BRANCH, Guangzhou, China; CSG POWER GENERATION CO., LTD MAINTENANCE AND TEST BRANCH, Guangzhou, China; CSG POWER GENERATION CO., LTD MAINTENANCE AND TEST BRANCH, Guangzhou, China; CSG POWER GENERATION CO., LTD MAINTENANCE AND TEST BRANCH, Guangzhou, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","2022 9th International Forum on Electrical Engineering and Automation (IFEEA)","10 Feb 2023","2022","","","6","10","With the development and reform of electric power enterprises, the demand for electric power inspection personnel in power plant enterprises has increased. However, power generation companies have problems such as different training needs for maintenance personnel, training methods that do not conform to enterprise development, and mismatch between teaching materials and training work. At the same time, the differences in employees’ own professional skills and ability to receive knowledge have resulted in low training efficiency. In order to solve this situation, this paper analyzes the shortcomings of the existing training methods, systems and standards for power plant maintenance personnel. The mixed reality augmented technology was proposed to improve the training efficiency of power plant maintenance personnel.","","978-1-6654-6421-5","10.1109/IFEEA57288.2022.10038214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038214","Mixed reality;maintenance training;power plants","Training;TV;Mixed reality;Maintenance engineering;Power systems;Personnel;Task analysis","augmented reality;electricity supply industry;inspection;maintenance engineering;personnel;power generation economics;pumped-storage power stations","different training needs;electric power enterprises;electric power inspection personnel;enterprise development;existing training methods;low training efficiency;maintenance training;mixed reality;power generation companies;power plant enterprises;power plant maintenance personnel;pumped storage power station;teaching materials;training work","","","","16","IEEE","10 Feb 2023","","","IEEE","IEEE Conferences"
"A protocol for visual cues in collaborative augmented reality applications","T. F. Emerenciano; L. V. Tenório; G. S. Moura; J. M. Teixeira; J. Kelner; I. Patriota; G. Almeida","Centro de Informática (CIn), Universidade Federal de Pernambuco, Recife, Pernambuco, Brazil; Centro de Informática (CIn), Universidade Federal de Pernambuco, Recife, Pernambuco, Brazil; Centro de Informática (CIn), Universidade Federal de Pernambuco, Recife, Pernambuco, Brazil; Centro de Informática (CIn), Universidade Federal de Pernambuco, Recife, Pernambuco, Brazil; Centro de Informática (CIn), Universidade Federal de Pernambuco, Recife, Pernambuco, Brazil; Companhia Hidro-Elétrica do São Francisco (CHESF), Recife, Pernambuco, Brazil; NA","2012 14th Symposium on Virtual and Augmented Reality","10 Sep 2012","2012","","","84","90","Inspection and maintenance of complex hardware equipment can have an elevated cost, due to the sophisticated training required for the technicians responsible for those tasks. The use of collaborative Augmented Reality can solve, or at least reduce, the cost of these activities. This paper proposes a protocol for the transmission of visual cues, represented by Augmented Reality elements, in collaborative applications. The protocol focuses on a client-server communication, where the client has the role to remotely supervise the activities, possibly sending interest points or instruction to the server. A prototype application was developed in order to demonstrate the advantages of a real application usage.","","978-1-4673-1929-4","10.1109/SVR.2012.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6297563","augmented reality;collaboration;communication protocol;remote;mobile device","Visualization;Augmented reality;Protocols;Collaboration;Image resolution;Quality of service;Tablet computers","augmented reality;client-server systems;computer based training;computer maintenance;groupware;inspection;protocols","collaborative augmented reality applications;complex hardware equipment maintenance;complex hardware equipment inspection;cost reduction;visual cues transmission protocol;client-server communication;remote supervision;training","","1","","12","IEEE","10 Sep 2012","","","IEEE","IEEE Conferences"
"Expansion augmented reality technology applied to the maintenance and diagnosis of equipment","W. -J. Shyr; C. -F. Chiou; C. -M. Lin","Department of Industrial Education and Technology, National Changhua University of Education, Changhua, Taiwan; Department of Industrial Education and Technology, National Changhua University of Education, Changhua, Taiwan; Department of Industrial Education and Technology, National Changhua University of Education, Changhua, Taiwan","2021 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS)","28 Dec 2021","2021","","","1","2","This study uses the expansion augmented reality technology combined with the virtual integration networking platform to realize the maintenance and diagnosis of equipment guarantee for engineers in the factory production line, as well as the establishment of the basic process of equipment operation, regular inspection of equipment to maintain the normal operation of production line equipment major tasks. Equipment operators in factory production lines are subject to rigorous education and training, with varying quality differences among employees, and some require experienced operators to be effective in achieving operational efficiency. In addition, in response to the rapid changes in the global pneumonia epidemic (COVID-19) and the mature development of Internet of Things technology to change the operating mode of most industries in Taiwan, the problem of personnel diversion to work groups, the data adjustment of traditional production line equipment parameters depended on paper records, data experience is not easy to pass on.","","978-1-6654-1951-2","10.1109/ISPACS51563.2021.9651035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9651035","augmented reality;virtual and real integration;internet of things;big data;pre-diagnosis and maintenance","Training;Image color analysis;Pulmonary diseases;Process control;Maintenance engineering;Signal processing;Production facilities","augmented reality;epidemics;inspection;Internet of Things;maintenance engineering;production engineering computing;production equipment","augmented reality technology;maintenance;virtual integration networking platform;equipment guarantee;factory production line;equipment operation;production line equipment parameters;global pneumonia epidemic;COVID-19;Internet of Things","","","","4","IEEE","28 Dec 2021","","","IEEE","IEEE Conferences"
"Development of travel speed detection method in welding simulator using augmented reality","A. S. Baskoro; I. Haryanto","Mechanical Engineering Departement, Universitas Indonesia Kampus UI Depok, Indonesia; Mechanical Engineering Departement, Universitas Indonesia Kampus UI Depok, Indonesia","2015 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","25 Feb 2016","2015","","","269","273","This paper explains about travel speed detection method that can be applied as the welding simulator using augmented reality. In welding process, the travel speed is an important parameter that influences the welding quality. In the future, this simulator can be used in welder training with a relatively low cost. This method uses ARToolkit, OpenGL library, and Autodesk 3Ds Max software for building the simulator. This method is the development of welding simulator that will show the deviation and accuracy of moving marker detection. This method uses differences in distance of the coordinate per unit time algorithm, taken from the amount of frames per second (FPS) of a camera. After this method was successfully built, the measurement data is taken to analyze the accuracy and the number of error in speed detection by the simulator from the actual speed with different light intensity parameter. The result is that the error in speed detection is not too large, so this simulator was successfully built and it can be developed further to get more sophisticated features on welding process in the future.","","978-1-5090-0363-1","10.1109/ICACSIS.2015.7415194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415194","Augmented Reality;Welding Simulator;Travel Speed","Welding;Cameras;Fluorescence;Databases;Light emitting diodes;Lighting;Documentation","augmented reality;automatic optical inspection;cameras;digital simulation;product quality;production engineering computing;velocity measurement;welding","travel speed detection method development;welding simulator;augmented reality;welding process;welding quality;welder training;ARToolkit;OpenGL library;Autodesk 3D Max software;moving marker detection;coordinate per unit time algorithm;frame per second;FPS;camera;measurement data analysis","","8","","7","IEEE","25 Feb 2016","","","IEEE","IEEE Conferences"
"[POSTER] Planning-Based Workflow Modeling for AR-enabled Automated Task Guidance","F. Han; J. Liu; W. Hoff; H. Zhang","Department of Computer Science, Colorado School of Mines, Golden, CO, USA; Department of Computer Science, Colorado School of Mines, Golden, CO, USA; Colorado School of Mines, Golden, CO, US; Department of Computer Science, Colorado School of Mines, Golden, CO, USA","2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)","30 Oct 2017","2017","","","58","62","In this paper, we implemented and validated a workflow modeling approach that is able to model a sequence of procedures to achieve a complex task to enable an AR-based automated task guidance system. We formulated automated task guidance as a decision making problem, based upon the general Partially Observable Markov Decision Processes (POMDP) paradigm as the foundation. Our approach is able to provide actionable information to actively instruct users through a complex multi-step task. Our method can also plan ahead an action sequence that is optimal in the long term, while maintaining flexibility to deal with changes in an uncertain environment. We validated our approach in the applications of copy machine inspection and compressor startup guidance. The experimental results have shown the effectiveness of our planning-based workflow models in real-world applications.","","978-0-7695-6327-5","10.1109/ISMAR-Adjunct.2017.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088449","Workflow modeling;augmented reality;task guidance","Inspection;Pulse width modulation;Hidden Markov models;Maintenance engineering;Computational modeling;Training data","augmented reality;compressors;decision making;inspection;Markov processes;mechanical engineering computing;planning (artificial intelligence);workflow management software","workflow modeling approach;complex task;decision making problem;general Partially Observable Markov Decision Processes paradigm;actionable information;complex multistep task;action sequence;compressor startup guidance;planning-based workflow models;AR-enabled automated task guidance system;copy machine inspection","","","","8","IEEE","30 Oct 2017","","","IEEE","IEEE Conferences"
"An Enhanced Photorealistic Immersive System using Augmented Situated Visualization within Virtual Reality","M. I. Iglesias; M. Jenkins; G. Morison","School of Engineering and Built Environment, Glasgow Caledonian Universit, Glasgow, United Kingdom; School of Engineering and Built Environment, Glasgow Caledonian Universit, Glasgow, United Kingdom; School of Engineering and Built Environment, Glasgow Caledonian Universit, Glasgow, United Kingdom","2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","6 May 2021","2021","","","514","515","This work presents a system which allows image data and extracted features from a real-world location to be captured and modelled in a Virtual Reality (VR) environment combined with Augmented Situated Visualizations (ASV) overlaid and registered in a virtual environment. Combining these technologies with techniques from Data Science and Artificial Intelligence (AI)(such as image analysis and 3D reconstruction) allows the creation of a setting where remote locations can be modelled and interacted with from anywhere in the world. This Enhanced Photorealistic Immersive (EPI) system is highly adaptable to a wide range of use cases and users as it can be utilized to model and interact with any environment which can be captured as image data (such as training for operation in hazardous environments, accessibility solutions for exploration of historical/tourism locations and collaborative learning environments). A use case example focused on a structural examination of railway tunnels along with a pilot study is presented, which can demonstrate the usefulness of the EPI system.","","978-1-6654-4057-8","10.1109/VRW52623.2021.00139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419272","Virtual reality;Situated visualization;Immersive analytics;Visual structural inspection","Training;Solid modeling;Adaptation models;Three-dimensional displays;Image analysis;Conferences;Virtual environments","computer aided instruction;feature extraction;groupware;image reconstruction;rendering (computer graphics);travel industry;virtual reality","EPI system;collaborative learning environments;hazardous environments;remote locations;image analysis;virtual environment;Augmented Situated Visualizations;Virtual Reality environment;real-world location;image data;Augmented Situated visualization;Enhanced Photorealistic Immersive system","","1","","6","IEEE","6 May 2021","","","IEEE","IEEE Conferences"
"A mobile application of augmented reality for aerospace maintenance training","T. Haritos; N. D. Macchiarella","Embrey-Riddle Aeronautical University, Daytona Beach, FL, USA; Embrey-Riddle Aeronautical University, Daytona Beach, FL, USA","24th Digital Avionics Systems Conference","27 Dec 2005","2005","1","","5.B.3","5.1","Aircraft maintenance technicians (AMTs) must obtain new levels of job task skill and knowledge to effectively work with modem computer-based avionics and advanced composite materials. Traditional methods of training, such as on-the-job training (OJT), may not have potential to fulfill the training requirements to meet future trends in aviation maintenance. A new instruction delivery system could assist AMTs with job task training and job tasks. The purpose of this research is to analyze the use of an augmented reality (AR) system as a training medium for novice AMTs. An AR system has the potential to enable job task training and job task guidance for the novice technician in a real world environment. An AR system could reduce the cost for training and retraining of AMTs by complementing human information processing and assisting with performance of job tasks. An AR system could eliminate the need to leave the aircraft for the retrieval of information from maintenance manuals for inspection and repair procedures. AR has the potential to supply rapid and accurate feedback to an AMT with any information that he/she needs to successfully complete a job task. New technologies that promote smaller computer-based systems make the application of a mobile AR system possible in the near future.","2155-7209","0-7803-9307-4","10.1109/DASC.2005.1563376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1563376","","Augmented reality;On the job training;Application software;Aerospace electronics;Aerospace materials;Aircraft manufacture;Modems;Composite materials;Costs;Humans","aircraft maintenance;computer based training;aerospace simulation;augmented reality;aerospace computing","mobile application;augmented reality;aerospace maintenance training;aircraft maintenance technicians;job task skill;computer-based avionics;advanced composite materials;on-the-job training;aviation maintenance;instruction delivery system;job task training;job task guidance;real world environment;human information processing;information retrieval;maintenance manuals;inspection procedure;repair procedure","","38","4","21","IEEE","27 Dec 2005","","","IEEE","IEEE Conferences"
"Application research and prospect of screenless display technologies in railway intelligent station","J. Duan; H. Shen; T. Shi; C. Li; G. Yang; Y. Chen","Institute of Computing Technology, China Academy of Railway Sciences Co. Ltd, Beijing, China; Institute of Computing Technology, China Academy of Railway Sciences Co. Ltd, Beijing, China; China Academy of Railway Sciences Co. Ltd, Beijing, China; Institute of Computing Technology, China Academy of Railway Sciences Co. Ltd, Beijing, China; Institute of Computing Technology, China Academy of Railway Sciences Co. Ltd, Beijing, China; Institute of Computing Technology, China Academy of Railway Sciences Co. Ltd, Beijing, China","2022 China Automation Congress (CAC)","13 Mar 2023","2022","","","6699","6702","With the development of display technology, screenless display technology represented by augmented reality (AR) technology, virtual reality (VR) technology, intelligent projection technology and (pseudo) holographic technology, in the construction of smart stations, can be used in passenger identity verification, equipment inspection, staff training, onsite navigation and passenger service. This paper first classifies and summarizes the technical principles of several screenless display technologies, and elaborates on specific technologies in detail. Subsequently, we have combed and summarized the application of several screenless display technologies in railway intelligent passenger stations in detail. Based on the actual demand of railway passenger stations, according to the current situation and limitations of the use of screen display technology in railway station equipment operation and maintenance and passenger services, we can try to study and apply new screenless display technologies, such as AR, VR and intelligent projection to improve the intelligence level of railway stations.","2688-0938","978-1-6654-6533-5","10.1109/CAC57257.2022.10055815","China Academy of Railway Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055815","Railway intelligent station;Screenless display;Augmented reality;Virtual reality;Intelligent projection technology","Training;Industries;Automation;Navigation;Inspection;Maintenance engineering;Rail transportation","augmented reality;display instrumentation;holography;inspection;maintenance engineering;railway engineering","AR technology;augmented reality;holographic technology;intelligence level;intelligent projection technology;onsite navigation;passenger services;railway intelligent passenger stations;railway station equipment operation;screen display technology;screenless display technology;staff training;virtual reality;VR technology","","","","5","IEEE","13 Mar 2023","","","IEEE","IEEE Conferences"
"Intelligent Itinerant Inspection Technology of Aircraft Based on Deep Learning and AR","Y. Xu; J. Tang; J. Zhou; Q. Cheng; J. Geng","Northwestern Polytechnical University, Xian, China; AVIC Chengdu Aircraft Industrial Group Company Ltd, Chengdu, China; AVICXI'AN Aircraft Industry Group Company Ltd, Xian, China; State-owned Wuhu Machinery Factory, Wuhu, China; Northwestern Polytechnical University, Xian, China","2022 5th World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM)","27 Jan 2023","2022","","","1023","1027","Itinerant inspection of aircraft is an important link to ensure the smooth flight of aircraft at high altitude. At present, the inspection of civil aviation aircraft mainly depends on human visual inspection, which is greatly affected by the environment, time-consuming and labor-intensive, and has many defects. To solve these problems, this paper proposes an intelligent itinerant inspection method based on deep learning and AR. Collect relevant pictures of aircraft defects and expand them to form a data set. Use the yolov5 network to train and learn the characteristics of various types of aircraft defects, form a specific depth learning model, and integrate it into AR glasses to identify aircraft defects. Use augmented reality to display the identification results. Finally, the Boeing 878 aircraft was taken as the object for experimental verification, and the aircraft defect recognition rate reached 80%, which verified the effectiveness of this aircraft intelligent itinerant technology.","","978-1-6654-7369-9","10.1109/WCMEIM56910.2022.10021333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021333","aircraft defect;augmented reality;deep learning","Deep learning;Visualization;Atmospheric modeling;Prototypes;Inspection;Real-time systems;Manufacturing","aircraft;aircraft control;augmented reality;automatic optical inspection;feature extraction;image recognition;inspection;learning (artificial intelligence)","aircraft defect recognition rate;aircraft defects;aircraft intelligent itinerant technology;Boeing 878 aircraft;civil aviation aircraft;deep learning;human visual inspection;intelligent itinerant inspection method;specific depth learning model","","","","8","IEEE","27 Jan 2023","","","IEEE","IEEE Conferences"
"Manufacture Assembly Fault Detection Method based on Deep Learning and Mixed Reality","S. Wang; R. Guo; H. Wang; Y. Ma; Z. Zong","Shenyang Institute of Computing Technology, Chinese Academy of Sciences Shenyang, Liaoning Province, China; Shenyang Institute of Computing Technology, Chinese Academy of Sciences Shenyang, Liaoning Province, China; Shenyang Institute of Computing Technology, Chinese Academy of Sciences Shenyang, Liaoning Province, China; Shenyang Institute of Computing Technology, Chinese Academy of Sciences Shenyang, Liaoning Province, China; Northeastern University","2018 IEEE International Conference on Information and Automation (ICIA)","26 Aug 2019","2018","","","808","813","Traditional manufacture assembly fault detection method more relied on manual operation leading to an ineffective procedure. In order to solve the problem making mistakes caused by manual inspection such as missing installations or lack of visual and intuitive guidance in traditional manufacturing assembly, A method of manufacture assembly fault detection based on deep learning and mixed reality is proposed. By training a pretreatment model and detecting targets via Faster R-CNN convolutional neural network, the accurate and efficient assembly detection is realized in the manufacturing by extracting feature information. Meanwhile, the mixed reality combination is realized by using the mixed reality device HoloLens as the human-computer interaction module to access the system. Augmented information interactions make the manufacturing and assembly inspection process more visual and intuitive. The experimental results show that the method proposed has a higher detection accuracy, a better user experience and a great capacity on an efficient assembly detection.","","978-1-5386-8069-8","10.1109/ICInfA.2018.8812577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812577","deep learning;Faster R-CNN;mixed reality;target detection","Virtual reality;Deep learning;Training;Manufacturing;Feature extraction;Solid modeling;Fault detection","assembling;augmented reality;convolutional neural nets;fault diagnosis;feature extraction;human computer interaction;inspection;learning (artificial intelligence);production engineering computing","deep learning;mixed reality combination;assembly inspection process;manufacturing assembly;assembly detection;mixed reality device;HoloLens;faster R-CNN convolutional neural network;manufacture assembly fault detection method;human-computer interaction module","","11","","12","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"DroneARchery: Human-Drone Interaction through Augmented Reality with Haptic Feedback and Multi-UAV Collision Avoidance Driven by Deep Reinforcement Learning","E. Dorzhieva; A. Baza; A. Gupta; A. Fedoseev; M. A. Cabrera; E. Karmanova; D. Tsetserukou","Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia; Skolkovo Institute of Science and Technology, Russia","2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","27 Dec 2022","2022","","","270","277","We propose a novel concept of augmented reality (AR) human-drone interaction driven by RL-based swarm behavior to achieve intuitive and immersive control of a swarm formation of unmanned aerial vehicles. The DroneARchery system developed by us allows the user to quickly deploy a swarm of drones, generating flight paths simulating archery. The haptic interface LinkGlide delivers a tactile stimulus of the bowstring tension to the forearm to increase the precision of aiming. The swarm of released drones dynamically avoids collisions between each other, the drone following the user, and external obstacles with behavior control based on deep reinforcement learning. The developed concept was tested in the scenario with a human, where the user shoots from a virtual bow with a real drone to hit the target. The human operator observes the ballistic trajectory of the drone in an AR and achieves a realistic and highly recognizable experience of the bowstring tension through the haptic display. The experimental results revealed that the system improves trajectory prediction accuracy by 63.3% through applying AR technology and conveying haptic feedback of pulling force. DroneARchery users highlighted the naturalness (4.3 out of 5 point Likert scale) and increased confidence (4.7 out of 5) when controlling the drone. We have designed the tactile patterns to present four sliding distances (tension) and three applied force levels (stiffness) of the haptic display. Users demonstrated the ability to distinguish tactile patterns produced by the haptic display representing varying bowstring tension(average recognition rate is of 72.8%) and stiffness (average recognition rate is of 94.2%). The novelty of the research is the development of an AR-based approach for drone control that does not require special skills and training from the operator. In the future, the proposed interaction can be applied in various fields, for example, for fast swarm deployment in search and rescue missions, crop monitoring, inspection and maintenance.","1554-7868","978-1-6654-5325-7","10.1109/ISMAR55827.2022.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9995060","Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices;Interaction paradigms;Mixed / augmented reality","Deep learning;Training;Force;Reinforcement learning;Haptic interfaces;Trajectory;Pattern recognition","augmented reality;autonomous aerial vehicles;collision avoidance;deep learning (artificial intelligence);feedback;haptic interfaces;mobile robots;multi-robot systems;reinforcement learning","AR;augmented reality;ballistic trajectory;behavior control;bowstring tension;deep reinforcement learning;drone control;DroneARchery system;DroneARchery users;fast swarm deployment;haptic display;haptic feedback;haptic interface LinkGlide;human operator;human operator observes;human-drone interaction;multiUAV collision avoidance;RL-based swarm behavior;swarm formation;tactile patterns;unmanned aerial vehicles","","1","","28","IEEE","27 Dec 2022","","","IEEE","IEEE Conferences"
"Robotic arm control using hybrid brain-machine interface and augmented reality feedback","Y. Wang; H. Zeng; A. Song; B. Xu; H. Li; L. Zhu; P. Wen; J. Liu","School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; AVIC Aeronautics Computing Technique Research Institute, Xi'an, China; Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET), Nanjing University of Information Sciences & Technology, Jiangsu","2017 8th International IEEE/EMBS Conference on Neural Engineering (NER)","14 Aug 2017","2017","","","411","414","Brain-machine interface (BMI) can be used to control robotic arm to assist paralysis people improving their quality of life. However process control of objects grasping is still a complex task for BMI users. High efficiency and accuracy is hard to achieve in objects grasping process even after extensive training. An important reason is lack of sufficient feedback information for performing the closed-loop control. In this study, we describe a method of augmented reality (AR) guiding assistance to provide extra feedback information to the user for closed-loop control. A hybrid BMI based system with AR feedback is proposed to evaluate the performance of our method in objects grasping task using robotic arm. Reaching and releasing tasks are completed by the robotic arm automatically. For the grasping task controlled by the user, AR is used to enrich the normal visual information during the grasping process to provide the BMI user augmented feedback information about the gripper status in real time. The feasibility of the proposed system both in open-loop (visual inspection) and closed-loop (AR feedback) are compared. According to our experimental results obtained from 5 subjects, the time used for controlling the robotic arm to grasp objects with AR feedback reduces more than 5s and the error rate of the gripper aperture decreases approximately 20% compared to those of grasping with normal visual inspection only. The results reveal that the BMI user can benefit from the information provided by AR interface in the grasping task.","1948-3554","978-1-5090-4603-4","10.1109/NER.2017.8008377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008377","","Grasping;Manipulators;Grippers;Visualization;Apertures;Electroencephalography","assisted living;augmented reality;brain-computer interfaces;closed loop systems;handicapped aids;medical disorders;medical robotics","robotic arm control;hybrid brain-machine interface;augmented reality feedback;paralysis;process control;grasping objects;BMI users;feedback information;AR guiding assistance;closed-loop control;reaching tasks;releasing tasks;normal visual inspection","","8","","13","IEEE","14 Aug 2017","","","IEEE","IEEE Conferences"
"The Application of Augmented Reality Technology in Urban Greening Plant Growth State Detection","H. Xing; F. Deng; Y. Tang; Q. Li; J. Zhang","College of Computer Science and Cyber Security, Chengdu University of Technology, Chengdu, China; College of Computer Science and Cyber Security, Chengdu University of Technology, Chengdu, China; College of Computer Science and Cyber Security, Chengdu University of Technology, Chengdu, China; Sichuan Tianyi Ecological Garden Group Company Ltd., Chengdu, China; China Southwest Architectural Design and Research Institute Company Ltd., Chengdu, China","IEEE Access","21 Jun 2023","2023","11","","59286","59296","The current target detection network in deep learning has been widely used in plant growth state detection. However, with the development of deep learning, within the field of plant growth state detection, the performance of the detection network is no longer the primary factor limiting the detection accuracy and model generalization ability. The construction of high-quality and large-scale plant datasets is more significant for the improvement of model detection accuracy and generalization ability. However, traditional methods for building deep learning datasets for plants have a large time span and low efficiency. And it is difficult to construct and expand the dataset for plants with complex growth environments and difficult image acquisition by existing methods. To address this problem, this paper proposes a method for constructing plant datasets based on augmented reality techniques. The method proposed in this paper allows for the rapid and efficient construction of large-scale field datasets that match the actual inspection environment in the lack of data. Meanwhile, this paper proposes an automatic annotation method for datasets in conjunction with the imaging environment in virtual space. In this paper, we experimentally compare the proposed method with the method of expanding the dataset using GAN networks. Using the virtual dataset constructed by the method proposed in this paper as the training set, the trained YOLOv5 model achieves an average accuracy (@0.5:0.95) of 0.71 for the three detection categories on the test set. The detection accuracy of the six mixed datasets constructed using the two data expansion methods on the test set was experimentally tested. The proposed method in this paper improved the accuracy by 2.2%, 3.1%, and 7.0%, respectively. The smaller the percentage of real images, the greater the accuracy improvement. Experiments show that the method proposed in this paper can well solve the problems faced in the field of plant growth state detection, such as the lack of data, and provides a new idea for the production and expansion of datasets in plant detection tasks.","2169-3536","","10.1109/ACCESS.2023.3284550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147199","Deep learning;object detection;plant growth state detection;augmented reality;YOLOv5;dataset augmentation;dataset construction","Training;Deep learning;Generative adversarial networks;Feature extraction;Task analysis;Object detection;Augmented reality","augmented reality;deep learning (artificial intelligence);feature extraction;learning (artificial intelligence);object detection","complex growth environments;current target detection network;deep learning datasets;detection categories;large-scale field datasets;large-scale plant datasets;model detection accuracy;plant detection tasks;urban greening plant growth state detection","","","","28","CCBYNCND","9 Jun 2023","","","IEEE","IEEE Journals"
"Mixed Reality Human Teleoperation","D. Black; S. Salcudean",University of British Columbia; University of British Columbia,"2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","1 May 2023","2023","","","375","383","For many applications, remote guidance and telerobotics provide great advantages. For example, tele-ultrasound can bring much-needed expert healthcare to isolated communities. However, existing tele-guidance methods have serious limitations. A new concept called human teleoperation leverages mixed reality, haptics, and high-speed communication to provide tele-guidance that is more tightly coupled than existing methods yet more accessible than telerobotics. This paper provides an overview of the human teleoperation concept and its application to tele-ultrasound. The concept and its impact are discussed, the graphics, communications, controls, and haptics subsystems are explained, and results are presented that show the system's efficacy. These include tests of the communication architecture, of human performance in tracking mixed reality signals, and of human teleoperation in a limited clinical use-case. The results show good potential for teleultrasound, as well as possible other applications of human teleoperation including remote maintenance, inspection, and training.","","979-8-3503-4839-2","10.1109/VRW58643.2023.00083","NSERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10108864","Human-centered computing—Human Computer Interaction—Interaction Paradigms—Mixed / augmented reality;Human-centered computing—Human Computer Interaction—Interaction Devices—Haptic devices;Computer systems organization—Embedded and cyber-physical systems—Robotics—Robotic Control","Training;Human computer interaction;Three-dimensional displays;Mixed reality;Medical services;Virtual reality;Maintenance engineering","biomedical ultrasonics;haptic interfaces;telemedicine;telerobotics;virtual reality","communication architecture;existing tele-guidance methods;expert healthcare;haptics subsystems;high-speed communication;human performance;human teleoperation concept;human teleoperation leverages mixed reality;isolated communities;mixed reality human teleoperation;mixed reality signals;remote guidance;serious limitations;tele-ultrasound;telerobotics;teleultrasound","","","","66","IEEE","1 May 2023","","","IEEE","IEEE Conferences"
"Modeling of video projectors in OpenGL for implementing a spatial augmented reality teaching system for assembly operations","C. M. Costa; G. Veiga; A. Sousa; L. Rocha; A. A. Sousa; R. Rodrigues; U. Thomas","Centre for Robotics in Industry and Intelligent Systems (CRIIS) of INESC TEC, Faculty of Engineering of the University of Porto, Portugal; Centre for Robotics in Industry and Intelligent Systems (CRIIS) of INESC TEC, Faculty of Engineering of the University of Porto, Portugal; Centre for Robotics in Industry and Intelligent Systems (CRIIS) of INESC TEC, Faculty of Engineering of the University of Porto, Portugal; Centre for Robotics in Industry and Intelligent Systems (CRIIS) of INESC TEC, Portugal; Centre for Information Systems and Computer Graphics (CSIG) of INESC TEC, Faculty of Engineering of the University of Porto, Portugal; Centre for Information Systems and Computer Graphics (CSIG) of INESC TEC, Faculty of Engineering of the University of Porto, Portugal; Robotics and Human Machine Interaction Laboratory at the Technical University of Chemnitz, Germany","2019 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)","10 Jun 2019","2019","","","1","8","Teaching complex assembly and maintenance skills to human operators usually requires extensive reading and the help of tutors. In order to reduce the training period and avoid the need for human supervision, an immersive teaching system using spatial augmented reality was developed for guiding inexperienced operators. The system provides textual and video instructions for each task while also allowing the operator to navigate between the teaching steps and control the video playback using a bare hands natural interaction interface that is projected into the workspace. Moreover, for helping the operator during the final validation and inspection phase, the system projects the expected 3D outline of the final product. The proposed teaching system was tested with the assembly of a starter motor and proved to be more intuitive than reading the traditional user manuals. This proof of concept use case served to validate the fundamental technologies and approaches that were proposed to achieve an intuitive and accurate augmented reality teaching application. Among the main challenges were the proper modeling and calibration of the sensing and projection hardware along with the 6 DoF pose estimation of objects for achieving precise overlap between the 3D rendered content and the physical world. On the other hand, the conceptualization of the information flow and how it can be conveyed on-demand to the operator was also of critical importance for ensuring a smooth and intuitive experience for the operator.","","978-1-7281-3558-8","10.1109/ICARSC.2019.8733617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733617","","Mathematical model;Three-dimensional displays;Education;Cameras;Matrix converters;Solid modeling;Robots","augmented reality;computer aided instruction;pose estimation;rendering (computer graphics);teaching","intuitive reality teaching application;projection hardware;video projectors;spatial augmented reality teaching system;assembly operations;teaching complex assembly;maintenance skills;human operators;extensive reading;training period;human supervision;immersive teaching system;inexperienced operators;textual instructions;video instructions;teaching steps;video playback;bare hands natural interaction interface;inspection phase;expected 3D outline;augmented reality teaching application;3D rendered content;6 DoF pose estimation","","3","","20","IEEE","10 Jun 2019","","","IEEE","IEEE Conferences"
"Reprojection error as a new metric to detect assembly/disassembly maintenance tasks","A. Rukubayihunga; J. -Y. Didier; S. Otmane",Wassa; IBISC Laboratory; IBISC Laboratory,"2015 International Conference on Image Processing Theory, Tools and Applications (IPTA)","4 Jan 2016","2015","","","513","518","Providing relevant information at the right time in the right place is the major challenge in augmented reality, especially when it is applied in industry related applications. Indeed, by superimposing virtual elements on images which capture the real scene, augmented reality has proved its potential and maturity for facilitating maintenance activities, especially in training, repairing or inspections. In this system, instructions for an assembly or disassembly actions are linearly displayed to users and triggered sequentially and manually by the operator once he has completed each individual step. In this paper, we explore a metric which will allow the system to determine automatically when two objects are assembled since it provides hints on the current step of the maintenance scenario. This metric is based on pose estimations and reprojection errors by considering that the two objects are independent. The first results obtained on both synthetic and real image sequences show that this metric is efficient in detecting assembly/disassembly instants. We also lend guidelines on how to integrate this metric in a bigger computer vision system designed around maintenance task scenarios provided using augmented reality.","2154-512X","978-1-4799-8637-8","10.1109/IPTA.2015.7367200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367200","3D computer vision;Error reprojection;Assembly;Disassembly;Augmented reality","Assembly;Three-dimensional displays;Maintenance engineering;Cameras;Solid modeling;Computational modeling;Measurement","assembling;augmented reality;computer vision;image capture;image sequences;maintenance engineering;pose estimation","reprojection error;disassembly maintenance task;augmented reality;virtual element;image capture;maintenance activity;pose estimation;image sequence;computer vision system;maintenance task scenario","","1","","24","IEEE","4 Jan 2016","","","IEEE","IEEE Conferences"
"Exploiting Mixed Reality in a Next-Generation IoT ecosystem of a construction site","T. Katika; F. K. Konstantinidis; T. Papaioannou; A. Dadoukis; S. N. Bolierakis; G. Tsimiklis; A. Amditis","Institute of Communication and Computer Systems, Athens, Greece, Zografou Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece, Zografou Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece, Zografou Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece, Zografou Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece, Zografou Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece, Zografou Athens, Greece; Institute of Communication and Computer Systems, Athens, Greece, Zografou Athens, Greece","2022 IEEE International Conference on Imaging Systems and Techniques (IST)","20 Jul 2022","2022","","","1","6","Ensuring smart safety of workers in large construction sites calls for centralized ecosystems, supported transversely by security, privacy, and trust enablers, to facilitate data sharing and protect the growing attack surface. At the same time, human-centricity has to be a core part of the process. Mixed Reality (MR) applications allow the execution of context-aware applications with advanced interaction interfaces improving decision making, data gathering, interoperability with other services, accessibility, and real-time data sharing. We leverage the advanced features of MR interfaces and interoperability with Internet of Things (IoT) systems to contribute towards a more usable, functional, and perceptive human-centric Next Generation ecosystem in large construction plants aiming to contribute to workers' health and safety (H&S) during inspection processes. In this study, an MR enabler is designed and developed to collect data from various sources of such an ecosystem. The main objectives are to identify workers in large construction sites and their medical and training records, get real-time information regarding their stress and health levels, and alert when dangerous activity is being performed, or a worker reaches an unauthorized location. At the same time, the MR enabler allows the visualization of BIM models, their digital representations and data, as well as the submission of reports. The MR enabler, connected to other components within the Next-Generation IoT ecosystem (NGIoT), will be applied to various construction sites in Poland.","1558-2809","978-1-6654-8102-1","10.1109/IST55454.2022.9827726","Horizon 2020(grant numbers:957258); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9827726","Mixed Reality;Health and Safety;Construction;IoT;Next-Generation;BIM","Training;Ecosystems;Mixed reality;Virtual reality;Real-time systems;Health and safety;Internet of Things","augmented reality;building information modelling;construction industry;Internet of Things;occupational health;occupational safety;occupational stress;open systems","mixed reality applications;context-aware applications;advanced interaction interfaces;Internet of Things systems;construction plants;MR enabler;next-generation IoT ecosystem;smart safety;centralized ecosystems;human-centric next generation ecosystem;worker health and safety;inspection processes;BIM models;NGIoT;Poland","","","","17","EU","20 Jul 2022","","","IEEE","IEEE Conferences"
"Automatic 2D-3D vision based assessment of the attitude of a train pantograph","E. Di Stefano; E. Ruffaldi; C. A. Avizzano","PERCeptual RObotics Laboratory, Scuola Superiore Sant'Anna; Scuola Superiore Sant’ Anna, PERCeptual RObotics Laboratory; Scuola Superiore Sant’ Anna, PERCeptual RObotics Laboratory","2016 IEEE International Smart Cities Conference (ISC2)","3 Oct 2016","2016","","","1","5","In this paper we propose an automatic visual based technique, integrated in a wayside monitoring system for train inspection, that allows to assess the attitude of the metal bow of a pantograph by combining a colour image captured by an RGB digital camera and a pointcloud built from a range sensor scan. An efficient and fast template-matching procedure allows to detect the pantograph in the scene and associate a matching attitude, searching for the most similar model present in a database. The record of templates belonging to the database exploits a virtual rendering environment that allows to optimize the training stage in terms of computational load and time. During actual inspection the RGB image and pointcloud of the pantograph are opportunely processed and aligned to the same reference frame. After the preliminary template-matching step, the pointcloud is augmented with the virtual model of the matched template and the attitude angular values are refined by applying the iterative closest point (ICP) algorithm between the real object and the virtual one, with the aim of reducing eventual residual errors.","","978-1-5090-1846-8","10.1109/ISC2.2016.7580747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7580747","","Cameras;Feature extraction;Solid modeling;Iterative closest point algorithm;Computational modeling;Visualization;Monitoring","cameras;image colour analysis;image matching;inspection;pantographs;railways;rendering (computer graphics);virtual reality","sensor scan;eventual residual error reduction;ICP algorithm;iterative closest point algorithm;reference frame;virtual rendering environment;matching attitude;RGB digital camera;colour image;point cloud;metal bow attitude assessment;train inspection;wayside monitoring system;train pantograph attitude;automatic 2D-3D vision based assessment","","9","","13","IEEE","3 Oct 2016","","","IEEE","IEEE Conferences"
"Improving Collocated Robot Teleoperation with Augmented Reality","H. Hedayati; M. Walker; D. Szafir","University of Colorado Boulder, Boulder, USA; University of Colorado Boulder, Boulder, USA; University of Colorado Boulder, Boulder, USA","2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI)","8 Jul 2021","2018","","","78","86","Robot teleoperation can be a challenging task, often requiring a great deal of user training and expertise, especially for platforms with high degrees-of-freedom (e.g., industrial manipulators and aerial robots). Users often struggle to synthesize information robots collect (e.g., a camera stream) with contextual knowledge of how the robot is moving in the environment. We explore how advances in augmented reality (AR) technologies are creating a new design space for mediating robot teleoperation by enabling novel forms of intuitive, visual feedback. We prototype several aerial robot teleoperation interfaces using AR, which we evaluate in a 48-participant user study where participants completed an environmental inspection task. Our new interface designs provided several objective and subjective performance benefits over existing systems, which often force users into an undesirable paradigm that divides user attention between monitoring the robot and monitoring the robot’s camera feed(s).","2167-2148","978-1-4503-4953-6","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9473794","Aerial robots;drones;teleoperation;augmented reality;mixed reality;interface design;aerial photography;free-flying robot","Visualization;Service robots;Robot vision systems;Cameras;Unmanned aerial vehicles;Space exploration;Task analysis","","","","","","40","","8 Jul 2021","","","IEEE","IEEE Conferences"
"Three-dimensional photometric reconstruction of a single view using machine learning techniques","L. Abada; S. Aouat; R. Elfani; Y. H. Zamoum","Artificial Intelligence and Data Science Department, Artifcial Intelligence Laboratory (LRIA), University of Sciences and Technology (USTHB), Algiers, Algeria; Artificial Intelligence and Data Science Department, Artifcial Intelligence Laboratory (LRIA), University of Sciences and Technology (USTHB), Algiers, Algeria; Artificial Intelligence and Data Science Department, Artifcial Intelligence Laboratory (LRIA), University of Sciences and Technology (USTHB), Algiers, Algeria; Artificial Intelligence and Data Science Department, Artifcial Intelligence Laboratory (LRIA), University of Sciences and Technology (USTHB), Algiers, Algeria","2022 7th International Conference on Image and Signal Processing and their Applications (ISPA)","3 Jun 2022","2022","","","1","6","In the computer vision field, detecting the structure and geometry of the three-dimensional world from two-dimensional images remains a big problem despite the research progressing rapidly. Three-dimensional (3D) reconstruction is the inverse problem of Image Formation, which consists to obtain a 3-dimensional representation of an object from one or more images and thus of recovering the lost dimension during the process of image formation. 3D reconstruction is very useful in many applications such as visual inspection of surfaces, augmented reality or also medical diagnostic assistance. There are several techniques that aim to reproduce human vision and build a 3D model. Among the 3D reconstruction techniques we find the shape from shading (SFS) which is known to be ill-posed since the solution is not unique. This forces us to model this problem in a different way so that it becomes well-posed. This pushes us to move to a more general method without constraints to reduce the complexity of the generation of 3D objects. This method is the Photometric Stereo (PS). In this paper, we propose a 3D reconstruction method based on machine learning with using different architecture and parameters.","","978-1-6654-8042-0","10.1109/ISPA54004.2022.9786340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786340","Photometric stereo;3D reconstruction;Neural Network;Machine learning;Shape From Shading","Training;Surface reconstruction;Solid modeling;Three-dimensional displays;Signal processing algorithms;Machine learning;Complexity theory","augmented reality;computer vision;image reconstruction;learning (artificial intelligence);stereo image processing","dimensional photometric reconstruction;computer vision field;geometry;three-dimensional world;two-dimensional images;research progressing;three-dimensional reconstruction;inverse problem;image formation;3-dimensional representation;lost dimension;augmented reality;medical diagnostic assistance;human vision;3D reconstruction techniques;Photometric Stereo;3D reconstruction method","","","","28","IEEE","3 Jun 2022","","","IEEE","IEEE Conferences"
"Mobile interactive hologram verification","A. Hartl; J. Grubert; D. Schmalstieg; G. Reitmayr",Graz University of Technology; Graz University of Technology; Graz University of Technology; Graz University of Technology,"2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","23 Dec 2013","2013","","","75","82","Verification of paper documents is an important part of checking a person's identity, authorization for access or simply establishing a trusted currency. Many documents such as passports or paper bills include holograms or other view-dependent elements that are difficult to forge and therefore are used to verify the genuineness of that document. View-dependent elements change their appearance based both on viewing direction and dominant light sources, thus it requires special knowledge and training to accurately distinguish original elements from forgeries. We present an interactive application for mobile devices that integrates the recognition of the documents with the interactive verification of view-dependent elements. The system recognizes and tracks the paper document, provides user guidance for view alignment and presents a stored image of the element's appearance depending on the current view of the document also recording user decisions. We describe how to model and capture the underlying spatially varying BRDF representation of view-dependent elements. Furthermore, we evaluate this approach within a user study and demonstrate that such a setup captures images that are recognizable and that can be correctly verified.","","978-1-4799-2869-9","10.1109/ISMAR.2013.6671766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671766","","Cameras;Mobile handsets;Light sources;Mobile communication;Security;Inspection;Light emitting diodes","document image processing;holography;interactive systems;mobile computing;mobile radio","view-dependent elements;document genuineness;viewing direction;light sources;forgeries;interactive application;mobile devices;documents recognition;user guidance;view alignment;BRDF representation;paper bills;passports;trusted currency;authorization;person identity checking;paper documents verification;mobile interactive hologram verification","","16","2","22","IEEE","23 Dec 2013","","","IEEE","IEEE Conferences"
"Flying head: A head-synchronization mechanism for flying telepresence","K. Higuchi; K. Fujii; J. Rekimoto","The Japan Society for the Promotion of Science, The University of Tokyo; The University of Tokyo; The University of Tokyo, Sony Computer Science Laboratory","2013 23rd International Conference on Artificial Reality and Telexistence (ICAT)","30 Jan 2014","2013","","","28","34","Flying Head is a telepresence system that remotely connects humans and unmanned aerial vehicles (UAVs). UAVs are teleoperated robots used in various situations, including disaster area inspection and movie content creation. This study aimed to integrate humans and machines with different abilities (i.e., flying) to virtually augment human abilities. Precise manipulation of UAVs normally involves simultaneous control of motion parameters and requires the skill of a trained operator. This paper proposes a new method that directly connects the user's body and head motion to that of the UAV. The user's natural movement can be synchronized with UAV motions such as rotation and horizontal and vertical movements. Users can control the UAV more intuitively since such manipulations are more in accordance with their kinesthetic imagery; in other words, a user can feel as if he or she became a flying machine.","","978-4-904490-11-2","10.1109/ICAT.2013.6728902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728902","5.2 [Information interfaces and presentation]: User Interfaces-User-centered design","Head;Robots;Cameras;Magnetic heads;Synchronization;Trajectory","aerospace control;autonomous aerial vehicles;mobile robots;telerobotics","head synchronization mechanism;flying head;flying telepresence;telepresence system;unmanned aerial vehicles;UAV;teleoperated robots;disaster area inspection;movie content creation;motion parameters;kinesthetic imagery;flying machine","","27","","31","","30 Jan 2014","","","IEEE","IEEE Conferences"
"Table of contents","",,"2014 IIAI 3rd International Conference on Advanced Applied Informatics","1 Dec 2014","2014","","","v","xix","The following topics are dealt with: data mining; Japanese WordNet synonym misplacement detection; social network; recommender system; sentiment analysis; workshop-based instruction; Japanese public libraries; machine learning methods; collaborative Web presentation support system; SMS4 ultracompact hardware implementation; wireless sensor networks; personalized public transportation recommendation system; adaptive user interface; NIS-Apriori algorithm; GetRNIA software tool; rough set-based rule generation; tree-Ga bump hunting; neural network model; weighted citation network analysis; sound proofing ventilation unit; touch interaction; mutually dependent Markov decision processes; ozone treatment; dynamic query optimization; big data; learner activity recognition; IoT-security approach; nutrition-based vegetable production; farm product cultivation; polynomial time mat learning; C-deterministic regular formal graph system; article abstract key expression extraction; English text comprehension; online social games; knowledge creation; knowledge utilization; online stock trading; customer behavior analysis; project-based collaborative learning; in-field mobile game-based learning activities; e-portfolio system design; self-regulated learning ontological model; mobile augmented reality based scaffolding platform; context-aware mobile Japanese conversation learning system; English writing error classification; image processing; outside-class learning; exercise-centric teaching materials; UML modeling; online historical document reading literacy; MMORPG-based learning environment; computer courses; undergraduate education; energy management system; higher education; decentralised auction-based bandwidth allocation; wireless networked control systems; resource scheduling algorithm; embedded cloud computing; Poisson distribution; Japanese seismic activity; suspect vehicle detection; 3D network traffic visualization; Web information retrieval; agent based disaster evacuation assist system; electroencephalogram; random number generator; multiagent simulations; multicore environment; CPU scheduler; multithreaded processes; reserve-price biddings; real-time traffic signal control; evolutionary computation; robot-assisted rehabilitation system; hybrid automata; Batik motif classification; color-texture-based feature extraction; backpropagation; multimedia storytelling; e-tourism service; Web mining; search engine; simulation-based e-learning mobile application software; library classification training system; WebQuest learning strategy; context-aware ubiquitous English learning; support vector machine; RFID tag ownership transfer protocol; cognitive linguistics; collaborative software engineering learning; write-access reduction method; NVM-DRAM hybrid memory; garbage collection; parallel indexing scheme; lazy-updating snoop cache protocol; distributed storage system; ITS application; software engineering education; ophthalmic multimodal imaging system; injected bug classification; secure live virtual machine migration; flash memory management; genetic programming; heterogeneous databases; time series similarity search; concurrency control program generation; incremental data migration; multidatabase system; software release time decision making; analytic hierarchy process; interactive genetic algorithm; biometric intelligence; talking robots; archaeological ruin analysis; GIS; optical wireless pedestrian-support systems; visual impairment; extreme programming; Japanese e-commerce Web sites; Chinese sign language animation; hearing-impaired people mammography inspection; geographical maps; electroculogram; XML element retrieval technique; image recognition; reinforcement learning; ECU formal verification; gasoline direct injection engines; earthquake disaster simulation; smart devices for autistic children; RoboCup rescue simulation; inductive logic programming; master-slave asynchronous evolutionary hybrid algorithm; VANET routing optimization; and Web image sharing services.","","978-1-4799-4173-5","10.1109/IIAI-AAI.2014.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913248","","","agriculture;analytic hierarchy process;archaeology;augmented reality;automata theory;backpropagation;bandwidth allocation;Big Data;biometrics (access control);cache storage;citation analysis;cloud computing;computational complexity;computer animation;computer games;computer science education;concurrency control;consumer behaviour;data mining;data visualisation;distributed databases;DRAM chips;educational courses;electroencephalography;electronic commerce;electro-oculography;emergency management;energy management systems;engines;feature extraction;flash memories;formal verification;further education;genetic algorithms;geographic information systems;groupware;handicapped aids;human computer interaction;humanoid robots;image classification;image colour analysis;image texture;inductive logic programming;intelligent tutoring systems;Internet of Things;investment;library automation;linguistics;mammography;Markov processes;medical robotics;mobile computing;multi-agent systems;multimedia computing;multiprocessing systems;multi-threading;natural language processing;networked control systems;neural nets;object detection;ozonation (materials processing);patient rehabilitation;pedestrians;Poisson distribution;processor scheduling;public transport;query processing;random number generation;recommender systems;rescue robots;resource allocation;rough set theory;search engines;security of data;seismology;social networking (online);software prototyping;software tools;stock markets;storage management;support vector machines;teaching;telecommunication network routing;text analysis;time series;traffic control;traffic engineering computing;travel industry;trees (mathematics);Unified Modeling Language;unsupervised learning;user interfaces;vehicular ad hoc networks;ventilation;virtual machines;wireless sensor networks;XML","data mining;Japanese WordNet synonym misplacement detection;social network;recommender system;sentiment analysis;workshop-based instruction;Japanese public libraries;machine learning methods;collaborative Web presentation support system;SMS4 ultracompact hardware implementation;wireless sensor networks;personalized public transportation recommendation system;adaptive user interface;NIS-Apriori algorithm;GetRNIA software tool;rough set-based rule generation;tree-Ga bump hunting;neural network model;weighted citation network analysis;sound proofing ventilation unit;touch interaction;mutually dependent Markov decision processes;ozone treatment;dynamic query optimization;big data;learner activity recognition;IoT-security approach;nutrition-based vegetable production;farm product cultivation;polynomial time mat learning;C-deterministic regular formal graph system;article abstract key expression extraction;English text comprehension;online social games;knowledge creation;knowledge utilization;online stock trading;customer behavior analysis;project-based collaborative learning;in-field mobile game-based learning activities;e-portfolio system design;self-regulated learning ontological model;mobile augmented reality based scaffolding platform;context-aware mobile Japanese conversation learning system;English writing error classification;image processing;outside-class learning;exercise-centric teaching materials;UML modeling;online historical document reading literacy;MMORPG-based learning environment;computer courses;undergraduate education;energy management system;higher education;decentralised auction-based bandwidth allocation;wireless networked control systems;resource scheduling algorithm;embedded cloud computing;Poisson distribution;Japanese seismic activity;suspect vehicle detection;3D network traffic visualization;Web information retrieval;agent based disaster evacuation assist system;electroencephalogram;random number generator;multiagent simulations;multicore environment;CPU scheduler;multithreaded processes;reserve-price biddings;real-time traffic signal control;evolutionary computation;robot-assisted rehabilitation system;hybrid automata;Batik motif classification;color-texture-based feature extraction;backpropagation;multimedia storytelling;e-tourism service;Web mining;search engine;simulation-based e-learning mobile application software;library classification training system;WebQuest learning strategy;context-aware ubiquitous English learning;support vector machine;RFID tag ownership transfer protocol;cognitive linguistics;collaborative software engineering learning;write-access reduction method;NVM-DRAM hybrid memory;garbage collection;parallel indexing scheme;lazy-updating snoop cache protocol;distributed storage system;ITS application;software engineering education;ophthalmic multimodal imaging system;injected bug classification;secure live virtual machine migration;flash memory management;genetic programming;heterogeneous databases;time series similarity search;concurrency control program generation;incremental data migration;multidatabase system;software release time decision making;analytic hierarchy process;interactive genetic algorithm;biometric intelligence;talking robots;archaeological ruin analysis;GIS;optical wireless pedestrian-support systems;visual impairment;extreme programming;Japanese e-commerce Web sites;Chinese sign language animation;hearing-impaired people mammography inspection;geographical maps;electrooculogram;XML element retrieval technique;image recognition;reinforcement learning;ECU formal verification;gasoline direct injection engines;earthquake disaster simulation;autistic children;RoboCup rescue simulation;inductive logic programming;master-slave asynchronous evolutionary hybrid algorithm;VANET routing optimization;Web image sharing services","","","","","IEEE","1 Dec 2014","","","IEEE","IEEE Conferences"
